{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HMHwfgowAYpu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import necessary items from Keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, UpSampling2D, ELU\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape, pool_size):\n",
        "    # Create the actual neural network here\n",
        "    model = Sequential()\n",
        "    # Normalizes incoming inputs. First layer needs the input shape to work\n",
        "    model.add(BatchNormalization(input_shape=input_shape))\n",
        "\n",
        "    # Below layers were re-named for easier reading of model summary; this not necessary\n",
        "    # Conv Layer 1\n",
        "    model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation=None, name = 'Conv1'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    \"\"\"\n",
        "    Note:\n",
        "    Batch normalization should be done before the activation since the pre-activation outputs are likely to\n",
        "    adhere to a gaussian distribution and thus benefit from the mean centering and variance scaling to eliminate\n",
        "    co-variate shift.\n",
        "    \"\"\"\n",
        "\n",
        "    # Conv Layer 2\n",
        "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Conv2'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "    # However, note that gradient computation will become more expensive, so this may not be a good fit\n",
        "    # for deployment on resource constrained devices, or for real time use cases that need low latency\n",
        "    # inference procedures\n",
        "\n",
        "    # Pooling 1\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "    # Conv Layer 3\n",
        "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Conv3'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "    # Dropout should be applied after batch normalization and non-linear activation\n",
        "\n",
        "    # Conv Layer 4\n",
        "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Conv4'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Layer 5\n",
        "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Conv5'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Pooling 2\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "    # Conv Layer 6\n",
        "    model.add(Conv2D(128, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Conv6'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Layer 7\n",
        "    model.add(Conv2D(128, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Conv7'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Pooling 3\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "    # Upsample 1\n",
        "    model.add(UpSampling2D(size=pool_size))\n",
        "\n",
        "    # Deconv 1\n",
        "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Deconv1'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 2\n",
        "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Deconv2'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Upsample 2\n",
        "    model.add(UpSampling2D(size=pool_size))\n",
        "\n",
        "    # Deconv 3\n",
        "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Deconv3'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 4\n",
        "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Deconv4'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 5\n",
        "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Deconv5'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Upsample 3\n",
        "    model.add(UpSampling2D(size=pool_size))\n",
        "\n",
        "    # Deconv 6\n",
        "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = None, name = 'Deconv6'))\n",
        "\n",
        "    # Adding batch normalization before applying non-linear activation function\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Using elu non-linearity since it can produce negative activations and it smoothes slowly\n",
        "    model.add(ELU(alpha=1.0))\n",
        "\n",
        "\n",
        "    # Final layer - only including one channel so 1 filter\n",
        "    model.add(Conv2DTranspose(1, (3, 3), padding='valid', strides=(1,1), activation = 'sigmoid', name = 'Final'))\n",
        "    # Using sigmoid for the final layer since the labels are between 0 and 1, which means our output activation\n",
        "    # should also be in this range\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "0eUf7g40AgyI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load training images\n",
        "    train_images = pickle.load(open(\"./full_CNN_train.p\", \"rb\" ))\n",
        "\n",
        "    # Load image labels\n",
        "    labels = pickle.load(open(\"./full_CNN_labels.p\", \"rb\" ))\n",
        "\n",
        "    # Make into arrays as the neural network wants these\n",
        "    train_images = np.array(train_images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Normalize labels - training images get normalized to start in the network\n",
        "    labels = labels / 255\n",
        "\n",
        "    # Shuffle images along with their labels, then split into training/validation sets\n",
        "    train_images, labels = shuffle(train_images, labels)\n",
        "    # Test size may be 10% or 20%\n",
        "    X_train, X_val, y_train, y_val = train_test_split(train_images, labels, test_size=0.1)\n",
        "\n",
        "    # Batch size, epochs and pool size below are all paramaters to fiddle with for optimization\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "    pool_size = (2, 2)\n",
        "    input_shape = X_train.shape[1:]\n",
        "\n",
        "    # Create the neural network\n",
        "    model = create_model(input_shape, pool_size)\n",
        "\n",
        "    # Using a generator to help the model use less data\n",
        "    # Channel shifts help with shadows slightly\n",
        "    datagen = ImageDataGenerator(channel_shift_range=0.2)\n",
        "    datagen.fit(X_train)\n",
        "\n",
        "    # Compiling and training the model\n",
        "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
        "    model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train)/batch_size,\n",
        "    epochs=epochs, verbose=1, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Freeze layers since training is done\n",
        "    model.trainable = False\n",
        "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
        "\n",
        "    # Save model architecture and weights\n",
        "    model.save('full_CNN_model.h5')\n",
        "\n",
        "    # Show summary of model\n",
        "    model.summary()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugHeU72fAjtB",
        "outputId": "fffc718f-513a-49a2-f66c-47fdcbbc5439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c49fb46255ca>:36: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train)/batch_size,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "89/89 [==============================] - 1805s 20s/step - loss: 0.0447 - val_loss: 0.3505\n",
            "Epoch 2/10\n",
            "89/89 [==============================] - 1827s 20s/step - loss: 0.0125 - val_loss: 0.0170\n",
            "Epoch 3/10\n",
            "89/89 [==============================] - 1796s 20s/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 4/10\n",
            "85/89 [===========================>..] - ETA: 1:34 - loss: 0.0092"
          ]
        }
      ]
    }
  ]
}