{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2y3Rcfl_bJto"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import necessary items from Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, UpSampling2D\n",
        "from keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4UNqu_XjbLWO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_model(input_shape, pool_size):\n",
        "    # Create the actual neural network here\n",
        "    model = Sequential()\n",
        "    # Normalizes incoming inputs. First layer needs the input shape to work\n",
        "    model.add(BatchNormalization(input_shape=input_shape))\n",
        "\n",
        "    # Below layers were re-named for easier reading of model summary; this not necessary\n",
        "    # Conv Layer 1\n",
        "    model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Conv1'))\n",
        "\n",
        "    # Conv Layer 2\n",
        "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Conv2'))\n",
        "\n",
        "    # Pooling 1\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "    # Conv Layer 3\n",
        "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Conv3'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Layer 4\n",
        "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Conv4'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Layer 5\n",
        "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Conv5'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Pooling 2\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "    # Upsample 1\n",
        "    model.add(UpSampling2D(size=pool_size))\n",
        "\n",
        "    # Deconv 1\n",
        "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Deconv1'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 2\n",
        "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Deconv2'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Upsample 2\n",
        "    model.add(UpSampling2D(size=pool_size))\n",
        "\n",
        "    # Deconv 3\n",
        "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Deconv3'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 4\n",
        "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Deconv4'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 5\n",
        "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Deconv5'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Upsample 3\n",
        "    model.add(UpSampling2D(size=pool_size))\n",
        "\n",
        "    # Deconv 6\n",
        "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Deconv6'))\n",
        "\n",
        "    # Final layer - only including one channel so 1 filter\n",
        "    model.add(Conv2DTranspose(1, (3, 3), padding='valid', strides=(1,1), activation = 'tanh', name = 'Final'))\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41SSd0cybQtQ",
        "outputId": "9baa604c-f423-444d-ffdb-d8ca318075d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-dacb18d754c8>:38: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train)/batch_size,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "89/89 [==============================] - 1267s 14s/step - loss: 0.0223 - val_loss: 0.0338\n",
            "Epoch 2/10\n",
            "89/89 [==============================] - 1220s 14s/step - loss: 0.0070 - val_loss: 0.0181\n",
            "Epoch 3/10\n",
            "89/89 [==============================] - 1222s 14s/step - loss: 0.0052 - val_loss: 0.0075\n",
            "Epoch 4/10\n",
            "89/89 [==============================] - 1212s 14s/step - loss: 0.0042 - val_loss: 0.0060\n",
            "Epoch 5/10\n",
            "89/89 [==============================] - 1211s 13s/step - loss: 0.0030 - val_loss: 0.0033\n",
            "Epoch 6/10\n",
            "89/89 [==============================] - 1215s 14s/step - loss: 0.0023 - val_loss: 0.0026\n",
            "Epoch 7/10\n",
            "89/89 [==============================] - 1217s 14s/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 8/10\n",
            "89/89 [==============================] - 1213s 14s/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 9/10\n",
            "89/89 [==============================] - 1217s 14s/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 10/10\n",
            "76/89 [========================>.....] - ETA: 3:00 - loss: 9.9385e-04"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    # Load training images\n",
        "    # train_images = pickle.load(open(\"/Users/divyamsobti10/Desktop/258/project/full_CNN_labels.p\", \"rb\" ))\n",
        "    train_images = pickle.load(open(\"/content/full_CNN_labels.p\", \"rb\" ))\n",
        "\n",
        "    # Load image labels\n",
        "    # labels = pickle.load(open(\"/Users/divyamsobti10/Desktop/258/project/full_CNN_labels.p\", \"rb\" ))\n",
        "    labels = pickle.load(open(\"/content/full_CNN_labels.p\", \"rb\" ))\n",
        "\n",
        "    # Make into arrays as the neural network wants these\n",
        "    train_images = np.array(train_images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Normalize labels - training images get normalized to start in the network\n",
        "    labels = labels / 255\n",
        "\n",
        "    # Shuffle images along with their labels, then split into training/validation sets\n",
        "    train_images, labels = shuffle(train_images, labels)\n",
        "    # Test size may be 10% or 20%\n",
        "    X_train, X_val, y_train, y_val = train_test_split(train_images, labels, test_size=0.1)\n",
        "\n",
        "    # Batch size, epochs and pool size below are all paramaters to fiddle with for optimization\n",
        "    batch_size = 128\n",
        "    epochs = 6\n",
        "    pool_size = (2, 2)\n",
        "    input_shape = X_train.shape[1:]\n",
        "\n",
        "    # Create the neural network\n",
        "    model = create_model(input_shape, pool_size)\n",
        "\n",
        "    # Using a generator to help the model use less data\n",
        "    # Channel shifts help with shadows slightly\n",
        "    datagen = ImageDataGenerator(channel_shift_range=0.2)\n",
        "    datagen.fit(X_train)\n",
        "\n",
        "    # Compiling and training the model\n",
        "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
        "    model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train)/batch_size,\n",
        "    epochs=epochs, verbose=1, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Freeze layers since training is done\n",
        "    model.trainable = False\n",
        "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
        "\n",
        "    # Save model architecture and weights\n",
        "    model.save('full_CNN_model.h5')\n",
        "\n",
        "    # Show summary of model\n",
        "    model.summary()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MZmpdSgbR20"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
